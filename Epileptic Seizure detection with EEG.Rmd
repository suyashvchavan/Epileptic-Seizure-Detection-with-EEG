---
title: "PROJECT FINAL"
author: "Suyashkumar Chavan"
date: "December 7, 2018"
output: pdf_document
---


```{r}
data = read.csv("AveragedData.csv", header = TRUE)
data = data[c(-1,-2)]
data$y[data$y != 1] = 0
#data$y = as.factor(data$y)
data[,1:178] = scale(data[,-179])
n = nrow(data)
smp_size = floor(0.75*n)
set.seed(123)
train_ind = sample(seq_len(n), size = smp_size)
train = data[train_ind,]
test = data[-train_ind,]
##visualising OGData
count = as.data.frame(table(train$y))
barplot(count$Freq, main="Frequency of Classes", names.arg = c("0","1"), xlab = "Class", ylab = "Count")

```


```{r}
#Lasso Regression
library(glmnet)
x = model.matrix(y~., data)[,-c(179)]
y = data$y
fit.cv = cv.glmnet(x,y,alpha = 1)
fit.cv$lambda.min
plot(fit.cv)
lasso.fit = glmnet(x,y,alpha=1, lambda = fit.cv$lambda.min)
abc = as.matrix(coef(lasso.fit)[,1])
```



```{r}
selecteddata = read.csv("selecteddata.csv", header = TRUE)
selecteddata$y[selecteddata$y != 1] = 0
set.seed(123)
train_ind = sample(seq_len(n), size = smp_size)
train = selecteddata[train_ind,]
test = selecteddata[-train_ind,]
```


```{r}
#LDA fitting
library(MASS)
library(caret)
lda.fit = lda(y~., data = train)

##Training error
lda.train = predict(lda.fit,train)
lda.class.train = lda.train$class
tr.lda.table = table(lda.class.train, train$y)
lda.train.error = mean(lda.class.train != train$y)

##prediction accuracy
lda.pred = predict(lda.fit,test)
lda.class = lda.pred$class
cm.lda = confusionMatrix(lda.class, as.factor(test$y))
print(cm.lda)
```


```{r}
#Classification Tree CHANGE TO FACTORS
library(tree)
library(ISLR)
set.seed(3)
train$y = as.factor(train$y)
test$y = as.factor(test$y)
tree.ep = tree(y~., data = train)
##Training Error


pred.tree.train = predict(tree.ep, data = train, type = "class")
table(pred.tree.train, train$y)
plot(tree.ep)
text(tree.ep, pretty = 0)
tree.train.error = mean(pred.tree.train !=train$y)
##Test Error
tree.pred.test = predict(tree.ep, test, type = "class")
cm.tree = confusionMatrix(tree.pred.test, test$y)
##Pruning
set.seed(3)
cv_tree = cv.tree(tree.ep, FUN = prune.misclass)
plot(cv_tree$size, cv_tree$dev, type = "b") ##shows us that tree with 9 nodes is best
prune.ep = prune.misclass(tree.ep, best = 12)
prune.pred = predict(prune.ep, test, type = "class")
Prune.error = mean(prune.pred != test$y)
plot(prune.ep)
text(prune.ep, pretty = 0)
#  Check
```


```{r}
library(randomForest)
# ## randpm forests (m = sqrt(p))
rf.tree = randomForest(y~., data = train, mtry = sqrt(178), ntree = 100, importance = TRUE)
pred.rf = predict(rf.tree, newdata = test)
cm.lda = confusionMatrix(pred.rf, as.factor(test$y))
print(cm.lda)

# Bagging
bag.tree = randomForest(y~., data = train, mtry = 178, ntree = 100, importance = TRUE)
pred.bag = predict(rf.tree, newdata = test)
cm.bag = confusionMatrix(pred.bag, as.factor(test$y))
print(cm.bag)
```

```{r}
# #KNN with k = 11
library(class)
train.x = train[-179]
test.x = test[-179]
train.y = train$y
train.y = as.factor(train$y)
test.y = as.factor(test$y)
data$y = as.factor(data$y)
# ##Finding best K for K Nearest Neighbors
trControl = trainControl(method = "cv", number = 10)
fit = train(y~., method = "knn", tuneGrid = expand.grid(k=1:11), trControl = trControl, metric = "Accuracy", data = data) ##Change to test data
# ##Predicting with KNN
pred.knn = knn(train.x, test.x, train.y, k = 4)
cm.knn = confusionMatrix(pred.knn, test.y)
print(cm.knn)
```

```{r}
# SVM
train$y = as.factor(train$y)
test$y = as.factor(test$y)
library(e1071)
svm.fit = svm(y~., data = train, cost = 0.1, degree = 1)
svmpredict = predict(svm.fit, test, type = "response")
cm.svm = confusionMatrix(svmpredict, test$y)
print(cm.svm)
```


```{r}
# Polynomical Kernl
train$y = as.factor(train$y)
test$y = as.factor(test$y)
library(e1071)
svm.poly = svm(y~., data = train, kernel = "polynomial", cost = 10, degree = 2)
svmpred.poly = predict(svm.poly, test, type = "response")
cm.poly = confusionMatrix(svmpred.poly, test$y)
print(cm.poly)
```



```{r}
# Radial Kernel
train$y = as.factor(train$y)
test$y = as.factor(test$y)
library(e1071)
svm.radial = svm(y~., data = train, kernel = "radial", cost = 1, gamma = 0.5)
svmpredict = predict(svm.radial, test, type = "response")
cm.svm = confusionMatrix(svmpredict, test$y)
print(cm.svm)
```

```{r}
# library(neuralnet)
# tt = cbind(train[-179], class.ind(train$y))
# nn = names(train[-179])
# nnn = names(train)
# ff = as.formula(paste("~ ",paste(nnn[!nnn %in% c("1", "0")], collapse = " + ")))
# f = as.formula(paste("y ~",paste(nn[!nn %in% c("1", "0")], collapse = " + ")))
# m = model.matrix(ff, data = train)
nn <- neuralnet(f, data=m, hidden = c(130,90,40,20,2),  linear.output=FALSE, threshold=0.01)
```


```{r}
nn.pred = compute(nn, test[-179])
prediction.nn = nn.pred$net.result
prediction.nn = ifelse(prediction.nn > 0.5, 1, 0)
MSE.nn = mean(prediction.nn == test$y)
table(prediction.nn, test$y)

roc = function(labels, scores){
  label = labels[order(scores, decreasing = TRUE)]
  data.frame(TPR = cumsum(labels)/sum(labels), FPR = cumsum(!labels)/sum(!labels), labels)
}
simple_roc = roc(test$y, prediction.nn)
plot(simple_roc[2:1], pch=".")
rm(list = setdiff(ls(), "data"))
```

```{r}
#Hierarchial clustering
#data=scale(data)
x=dist(data,method="euclidean")
hc.complete=hclust(x,method="complete")
hc.average =hclust(x, method ="average")
hc.single =hclust(x, method ="single")
par(mfrow =c(1,3))
plot(hc.complete ,main =" Complete Linkage ", xlab="", sub ="",cex =.9, hang =-1)
group.complete=cutree(hc.complete,k=50)
rect.hclust(hc.complete,k=50,border="blue")
plot(hc.average , main =" Average Linkage ", xlab="", sub ="",cex =.9, hang=-1)
group.complete=cutree(hc.average,k=50)
rect.hclust(hc.average,k=50,border="red")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,hang=-1)
group.complete=cutree(hc.single,k=50)
rect.hclust(hc.single,k=50,border="green")
#K means clustering
set.seed(123) #Set the seed for reproducibility
k=kmeans(data[,-c(179)], centers=5, iter.max=10,nstart=20) #Create 5 clusters, Remove columns 179
k$centers #Display&nbsp;cluster centers
table(k$cluster) #Give a count of data points in each cluster
k.range=2:20 #K from 2 to 20
tries=100 #Run the K Means algorithm 100 times
avg.totw.ss=integer(length(k.range)) #Set up an empty vector to hold all of points
for(v in k.range){ # For each value of the range variable
 v.totw.ss=integer(tries) #Set up an empty vector to hold the 100 tries
 for(i in 1:tries){
 k.temp=kmeans(data,centers=v) #Run kmeans
 v.totw.ss[i]=k.temp$tot.withinss#Store the total withinss
 }
 avg.totw.ss[v-1]=mean(v.totw.ss) #Average the 100 total withinss
}
plot(k.range,avg.totw.ss,type="b", main="Total Within SS by Various K",ylab="Average Total Within Sum of Squares",
xlab="Value of K")
```
```{r}

```

